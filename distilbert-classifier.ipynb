{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-06T20:44:18.459426Z","iopub.status.busy":"2023-12-06T20:44:18.458499Z","iopub.status.idle":"2023-12-06T20:44:18.465146Z","shell.execute_reply":"2023-12-06T20:44:18.464186Z","shell.execute_reply.started":"2023-12-06T20:44:18.459388Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import DistilBertModel, DistilBertTokenizer\n","from torch import cuda\n","\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(f'Runs on {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:44:38.127939Z","iopub.status.busy":"2023-12-06T20:44:38.127537Z","iopub.status.idle":"2023-12-06T20:44:38.132903Z","shell.execute_reply":"2023-12-06T20:44:38.131924Z","shell.execute_reply.started":"2023-12-06T20:44:38.127907Z"},"trusted":true},"outputs":[],"source":["MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 8\n","VALID_BATCH_SIZE = 20\n","EPOCHS = 5\n","LEARNING_RATE = 1e-05\n","THRESHOLD = 0.5\n","TRAIN_BACKBONE = True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:44:42.048477Z","iopub.status.busy":"2023-12-06T20:44:42.047538Z","iopub.status.idle":"2023-12-06T20:45:24.500032Z","shell.execute_reply":"2023-12-06T20:45:24.499114Z","shell.execute_reply.started":"2023-12-06T20:44:42.048438Z"},"trusted":true},"outputs":[],"source":["!pip install wandb\n","import wandb\n","\n","wandb.login()\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"wat-distilbert\",\n","    \n","    # track hyperparameters and run metadata\n","    config={\n","        \"learning_rate\": LEARNING_RATE,\n","        \"architecture\": \"distilbert\",\n","        \"epochs\": EPOCHS,\n","        \"train_distilbert\": TRAIN_BACKBONE,\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:45:45.481112Z","iopub.status.busy":"2023-12-06T20:45:45.480719Z","iopub.status.idle":"2023-12-06T20:45:45.569788Z","shell.execute_reply":"2023-12-06T20:45:45.568649Z","shell.execute_reply.started":"2023-12-06T20:45:45.481077Z"},"trusted":true},"outputs":[],"source":["id2label = {0: \"Irrelevant\", 1: \"Relevant\"}\n","label2id = {\"Irrelevant\": 0, \"Relevant\": 1}\n","\n","df = pd.read_csv('../data/WaTA_dataset.csv', encoding = \"ISO-8859-1\")\n","df['Class'] = df['Class'].apply(label2id.get)\n","df.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:45:49.618726Z","iopub.status.busy":"2023-12-06T20:45:49.618339Z","iopub.status.idle":"2023-12-06T20:45:49.631032Z","shell.execute_reply":"2023-12-06T20:45:49.629387Z","shell.execute_reply.started":"2023-12-06T20:45:49.618679Z"},"trusted":true},"outputs":[],"source":["count = df['Class'].value_counts()\n","\n","print(f'Number of irrelevant sentences: {count[0]}')\n","print(f'Number of relevant sentences: {count[1]}')\n","print(f'Percentage of irrelevant: {count[0] / (count[0] + count[1])}')\n","print(f'Percentage of relevant: {count[1] / (count[0] + count[1])}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:45:55.111227Z","iopub.status.busy":"2023-12-06T20:45:55.110296Z","iopub.status.idle":"2023-12-06T20:45:55.121465Z","shell.execute_reply":"2023-12-06T20:45:55.120407Z","shell.execute_reply.started":"2023-12-06T20:45:55.111186Z"},"trusted":true},"outputs":[],"source":["def tokenize(sentence, tokenizer):\n","    inputs = tokenizer.encode_plus(\n","        sentence,\n","        None,\n","        add_special_tokens=True,\n","        max_length=MAX_LEN,\n","        padding='max_length',\n","        truncation=True,\n","        return_token_type_ids=True\n","    )\n","    ids = inputs['input_ids']\n","    mask = inputs['attention_mask']\n","    return torch.tensor(ids, dtype=torch.long), torch.tensor(mask, dtype=torch.long)\n","\n","class RelevanceDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.len = len(data)\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        \n","    def __getitem__(self, index):\n","        sentence = self.data.Sentence[index]\n","        \n","        ids, mask = tokenize(sentence, self.tokenizer)\n","\n","        return {\n","            'ids': ids,\n","            'mask': mask,\n","            'sentence': sentence,\n","            'targets': torch.tensor(self.data.Class[index], dtype=torch.float)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:07.493412Z","iopub.status.busy":"2023-12-06T20:46:07.492558Z","iopub.status.idle":"2023-12-06T20:46:07.703946Z","shell.execute_reply":"2023-12-06T20:46:07.702751Z","shell.execute_reply.started":"2023-12-06T20:46:07.493379Z"},"trusted":true},"outputs":[],"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n","\n","train_size = 0.9\n","train_dataset = df.sample(frac=train_size,random_state=200)\n","test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","\n","print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","training_set = RelevanceDataset(train_dataset, tokenizer)\n","testing_set = RelevanceDataset(test_dataset, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:19.831140Z","iopub.status.busy":"2023-12-06T20:46:19.830415Z","iopub.status.idle":"2023-12-06T20:46:19.837778Z","shell.execute_reply":"2023-12-06T20:46:19.836541Z","shell.execute_reply.started":"2023-12-06T20:46:19.831106Z"},"trusted":true},"outputs":[],"source":["train_params = {\n","    'batch_size': TRAIN_BATCH_SIZE,\n","    'shuffle': True,\n","    'num_workers': 0\n","}\n","\n","test_params = {\n","    'batch_size': VALID_BATCH_SIZE,\n","    'shuffle': True,\n","    'num_workers': 0\n","}\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:32.781811Z","iopub.status.busy":"2023-12-06T20:46:32.781113Z","iopub.status.idle":"2023-12-06T20:46:32.790560Z","shell.execute_reply":"2023-12-06T20:46:32.789524Z","shell.execute_reply.started":"2023-12-06T20:46:32.781779Z"},"trusted":true},"outputs":[],"source":["class BinaryClassifier(torch.nn.Module):\n","    def __init__(self):\n","        super(BinaryClassifier, self).__init__()\n","        self.backbone = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","        self.head = torch.nn.Sequential(\n","            torch.nn.Linear(768, 768),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(768, 1),\n","            torch.nn.Sigmoid()\n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        backbone_out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = backbone_out[0]\n","        return self.head(hidden_state[:, 0])\n","    \n","    def unfreeze_backbone(self):\n","        for param in self.backbone.parameters():\n","            param.requires_grad = True\n","            \n","    def freeze_backbone(self):\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:35.584492Z","iopub.status.busy":"2023-12-06T20:46:35.584115Z","iopub.status.idle":"2023-12-06T20:46:37.690357Z","shell.execute_reply":"2023-12-06T20:46:37.689370Z","shell.execute_reply.started":"2023-12-06T20:46:35.584460Z"},"trusted":true},"outputs":[],"source":["model = BinaryClassifier()\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:38:22.276327Z","iopub.status.busy":"2023-12-06T20:38:22.275387Z","iopub.status.idle":"2023-12-06T20:38:27.887942Z","shell.execute_reply":"2023-12-06T20:38:27.887041Z","shell.execute_reply.started":"2023-12-06T20:38:22.276283Z"},"trusted":true},"outputs":[],"source":["# Run this cell if you want to use pretrained classification model\n","# saved_model = wandb.restore('classifier.bin', run_path=\"wat-distilbert/ru3wl9xi\")8p404q8d\n","saved_model = wandb.restore('classifier.bin', run_path=\"wat-distilbert/8p404q8d\")\n","#saved_tokenizer = wandb.restore('tokenizer.bin', run_path=\"wat-distilbert/ru3wl9xi\")\n","model = torch.load(saved_model.name)\n","model.to(device)\n","#tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n","#loaded_tokenizer = loaded_tokenizer.load_vocabulary(saved_tokenizer.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:46.151917Z","iopub.status.busy":"2023-12-06T20:46:46.151446Z","iopub.status.idle":"2023-12-06T20:46:46.162355Z","shell.execute_reply":"2023-12-06T20:46:46.159039Z","shell.execute_reply.started":"2023-12-06T20:46:46.151883Z"},"trusted":true},"outputs":[],"source":["loss_function = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:49.437739Z","iopub.status.busy":"2023-12-06T20:46:49.437323Z","iopub.status.idle":"2023-12-06T20:46:49.445924Z","shell.execute_reply":"2023-12-06T20:46:49.444907Z","shell.execute_reply.started":"2023-12-06T20:46:49.437682Z"},"trusted":true},"outputs":[],"source":["def compute_accuracy(tp, tn, fp, fn):\n","    return (tp + tn) / (tp + tn + fp + fn)\n","\n","def compute_precision(tp, fp):\n","    return tp / (tp + fp)\n","\n","def compute_recall(tp, fn):\n","    return tp / (tp + fn)\n","\n","def compute_f1(tp, fn, fp):\n","    return tp / (tp + (fn + fp) / 2)\n","\n","def pred_to_class(pred, threshold=0.5):\n","    return (pred >= THRESHOLD).float() "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:46:52.633067Z","iopub.status.busy":"2023-12-06T20:46:52.632188Z","iopub.status.idle":"2023-12-06T20:46:55.066898Z","shell.execute_reply":"2023-12-06T20:46:55.065742Z","shell.execute_reply.started":"2023-12-06T20:46:52.633030Z"},"trusted":true},"outputs":[],"source":["def test_example(model, tokenizer, nb_relevant=20, nb_irrelevant=20):\n","    for i in range(len(testing_set)):\n","        test_data = testing_set[i]\n","        ids, mask, sentence, target = test_data['ids'], test_data['mask'], test_data['sentence'], test_data[\"targets\"]\n","        if target.item() == 1:\n","            if nb_relevant > 0:\n","                nb_relevant -= 1\n","            else:\n","                continue\n","        elif target.item() == 0:\n","            if nb_irrelevant > 0:\n","                nb_irrelevant -= 1\n","            else:\n","                continue\n","        ids = ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        outputs = model(ids, mask)\n","        pred_class = pred_to_class(outputs, THRESHOLD)\n","        print(sentence)\n","        print(f\"pred: {id2label[pred_class.item()]}, target: {id2label[target.item()]}\")\n","    \n","test_example(model, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T20:47:26.978421Z","iopub.status.busy":"2023-12-06T20:47:26.977450Z","iopub.status.idle":"2023-12-06T21:40:34.989266Z","shell.execute_reply":"2023-12-06T21:40:34.988060Z","shell.execute_reply.started":"2023-12-06T20:47:26.978383Z"},"trusted":true},"outputs":[],"source":["# TRAINING\n","\n","if TRAIN_BACKBONE:\n","    model.unfreeze_backbone()\n","\n","for i in range(EPOCHS):\n","    total_loss = 0\n","    nb_steps = 0\n","    tp, fp, fn, tn = 0, 0, 0, 0 \n","    model.train()\n","    for j,data in enumerate(training_loader):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float).reshape(-1, 1)\n","\n","        outputs = model(ids, mask)\n","        loss = loss_function(outputs, targets)\n","        total_loss += loss.item()\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        pred_class = pred_to_class(outputs, THRESHOLD)\n","        t_preds, t_targets = pred_class == 1, targets == 1\n","        f_preds, f_targets = pred_class == 0, targets == 0\n","        tp += (t_preds & t_targets).sum().item()\n","        fp += (t_preds & f_targets).sum().item()\n","        fn += (f_preds & t_targets).sum().item()\n","        tn += (f_preds & f_targets).sum().item()\n","        \n","        nb_steps += 1\n","        \n","        if j > 0 and j % 500 == 0:\n","            print(f\"Training Loss per 500 steps: {loss.item()}\")\n","            print(f\"Training Accuracy per 500 steps: {compute_accuracy(tp, tn, fp, fn)}\")\n","            print(f\"Training Precision per 500 steps: {compute_precision(tp, fp)}\")\n","            print(f\"Training Recall per 500 steps: {compute_recall(tp, fn)}\")\n","            print(f\"Training f1 per 500 steps: {compute_f1(tp, fn, fp)}\")\n","    \n","    avg_loss = total_loss / nb_steps\n","    accuracy = compute_accuracy(tp, tn, fp, fn)\n","    precision = compute_precision(tp, fp)\n","    recall = compute_recall(tp, fn)\n","    f1 = compute_f1(tp, fn, fp)\n","    wandb.log({\"loss\": avg_loss, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1})\n","    print(f\"Epoch {i}, avg loss: {avg_loss}\")\n","    print(f\"Epoch {i}, accuracy: {accuracy}\")\n","    print(f\"Epoch {i}, precision: {precision}\")\n","    print(f\"Epoch {i}, recall: {recall}\")\n","    print(f\"Epoch {i}, f1: {f1}\")\n","    \n","    test_example(model, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T21:42:15.188928Z","iopub.status.busy":"2023-12-06T21:42:15.188568Z","iopub.status.idle":"2023-12-06T21:42:39.221818Z","shell.execute_reply":"2023-12-06T21:42:39.221009Z","shell.execute_reply.started":"2023-12-06T21:42:15.188900Z"},"trusted":true},"outputs":[],"source":["# EVALUATION\n","\n","model.eval()\n","tp, fp, fn, tn = 0, 0, 0, 0\n","total_loss = 0\n","nb_steps = 0\n","with torch.no_grad():\n","    for i, data in enumerate(testing_loader):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float).reshape(-1, 1)\n","        \n","        outputs = model(ids, mask)\n","        loss = loss_function(outputs, targets)\n","        total_loss += loss.item()\n","        \n","        pred_class = pred_to_class(outputs, THRESHOLD)\n","        t_preds, t_targets = pred_class == 1, targets == 1\n","        f_preds, f_targets = pred_class == 0, targets == 0\n","        tp += (t_preds & t_targets).sum().item()\n","        fp += (t_preds & f_targets).sum().item()\n","        fn += (f_preds & t_targets).sum().item()\n","        tn += (f_preds & f_targets).sum().item()\n","\n","        nb_steps += 1\n","\n","        if i % 100 == 0:\n","            print(f\"Validation Loss per 100 steps: {loss.item()}\")\n","            print(f\"Validation Accuracy per 100 steps: {compute_accuracy(tp, tn, fp, fn)}\")\n","            print(f\"Validation Precision per 100 steps: {compute_precision(tp, fp)}\")\n","            print(f\"Validation Recall per 100 steps: {compute_recall(tp, fn)}\")\n","            print(f\"Validation f1 per 500 steps: {compute_f1(tp, fn, fp)}\")\n","\n","avg_loss = total_loss / nb_steps\n","accuracy = compute_accuracy(tp, tn, fp, fn)\n","precision = compute_precision(tp, fp)\n","recall = compute_recall(tp, fn)\n","f1 = compute_f1(tp, fn, fp)\n","print(f\"Avg loss: {avg_loss}\")\n","print(f\"Accuracy: {accuracy}\")\n","print(f\"Precision: {precision}\")\n","print(f\"Recall: {recall}\")\n","print(f\"f1: {f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T21:42:08.885871Z","iopub.status.busy":"2023-12-06T21:42:08.885463Z","iopub.status.idle":"2023-12-06T21:42:09.755095Z","shell.execute_reply":"2023-12-06T21:42:09.754119Z","shell.execute_reply.started":"2023-12-06T21:42:08.885837Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","out_model = os.path.join(wandb.run.dir, \"classifier.bin\")\n","out_tokenizer = os.path.join(wandb.run.dir, \"tokenizer.bin\")\n","\n","torch.save(model, out_model)\n","tokenizer.save_vocabulary(out_tokenizer)\n","\n","wandb.save('classifier.bin')\n","wandb.save('tokenizer.bin')\n","\n","out_model_pt = os.path.join(wandb.run.dir, \"classifier.pt\")\n","torch.save(model.state_dict(), out_model_pt)\n","\n","wandb.save('classifier.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T21:44:20.368670Z","iopub.status.busy":"2023-12-06T21:44:20.367854Z","iopub.status.idle":"2023-12-06T21:44:22.657126Z","shell.execute_reply":"2023-12-06T21:44:22.656182Z","shell.execute_reply.started":"2023-12-06T21:44:20.368634Z"},"trusted":true},"outputs":[],"source":["test_example(model, tokenizer, 10, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-06T21:44:14.807118Z","iopub.status.busy":"2023-12-06T21:44:14.806745Z","iopub.status.idle":"2023-12-06T21:44:17.696667Z","shell.execute_reply":"2023-12-06T21:44:17.695824Z","shell.execute_reply.started":"2023-12-06T21:44:14.807089Z"},"trusted":true},"outputs":[],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["THRESHOLD TESTS\n","\n","THRESHOLD: 0.5\n","\n","Avg loss: 0.7038330654244102 \\\n","Accuracy: 0.8570290720828355 \\\n","Precision: 0.8728854519091348 \\\n","Recall: 0.9495268138801262 \\\n","f1: 0.9095945605640896\n","\n","(icy-silence-9) \\\n","Avg loss: 0.831273452559438 \\\n","Accuracy: 0.8410991636798089 \\\n","Precision: 0.9219539584503088 \\\n","Recall: 0.8633017875920084 \\\n","f1: 0.8916644040184633\n","\n","(different-dream-10) \\\n","Avg loss: 0.5872668912708168 \\\n","Accuracy: 0.8614097968936678 \\\n","Precision: 0.8940162271805274 \\\n","Recall: 0.926919032597266 \\\n","f1: 0.9101703665462054\n","\n","(worthy-breeze-12) \\\n","Avg loss: 0.9387986349253287 \\\n","Accuracy: 0.8566308243727598 \\\n","Precision: 0.8933673469387755 \\\n","Recall: 0.9206098843322819 \\\n","f1: 0.9067840497151735\n","\n","(frosty-star-13) \\\n","Avg loss: 0.3630746433008758 \\\n","Accuracy: 0.8546395858223815 \\\n","Precision: 0.881011403073872 \\\n","Recall: 0.9342797055730809 \\\n","f1: 0.9068639959173258\n","\n","THRESHOLD: 0.6\n","\n","Avg loss: 0.7061429985850636 \\\n","Accuracy: 0.8554360812425329 \\\n","Precision: 0.8759159745969711 \\\n","Recall: 0.9426919032597266 \\\n","f1: 0.9080779944289693\n","\n","THRESHOLD: 0.7\n","\n","Avg loss: 0.7037493148331239 \\\n","Accuracy: 0.8566308243727598 \\\n","Precision: 0.8801775147928994 \\\n","Recall: 0.9384858044164038 \\\n","f1: 0.9083969465648855\n","\n","THRESHOLD: 0.8\n","\n","Avg loss: 0.7040119320611536 \\\n","Accuracy: 0.8566308243727598 \\\n","Precision: 0.8847305389221557 \\\n","Recall: 0.9321766561514195 \\\n","f1: 0.9078341013824884\n","\n","**THRESHOLD: 0.9**\n","\n","Avg loss: 0.7037559155104978 \\\n","Accuracy: 0.8610115491835922 \\\n","Precision: 0.8955680081507896 \\\n","Recall: 0.9242902208201893 \\\n","f1: 0.9097024579560156\n","\n","THRESHOLD: 0.95\n","\n","Avg loss: 0.7070154592197108 \\\n","Accuracy: 0.8582238152130626 \\\n","Precision: 0.9017671517671517 \\\n","Recall: 0.9121976866456362 \\\n","f1: 0.9069524307370622"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4115314,"sourceId":7132584,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
